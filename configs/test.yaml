random_seed: 42
use_gpu: true
dummy_data_train_samples: 500 # Reduced for faster CV example
dummy_data_val_samples: 100   # This will be our "test set" for base models
dummy_data_features: 10       # Features for tabular models (LGBM)
dummy_data_classes: 2         # Binary classification for simplicity
dummy_data_train_weights:     # Imbalanced
- 0.9
- 0.1
dummy_data_val_weights:       # Imbalanced
- 0.8
- 0.2

balancing:
  use_rsmote_gan_in_cv: true # Apply RSMOTE within CV folds
  rsmote_gan_params: # As per spec for AUROC
    k: 5
    minority_upsample_factor: 3.0

loss_function: # For DL models if trained directly (not part of ensemble CV here)
                # This section is less used by the ensemble script directly but good for full config
  type: ClassBalancedFocalLoss
  beta: 0.9999 # As per spec for AUROC
  gamma: 2.0    # As per spec for AUROC

optimizer: # For DL models
  name: Adam
  lr: 0.001 # Default, can be tuned

training: # General params, less relevant for CV-based ensemble script's main flow
  num_epochs: 5 # Default, can be tuned (used by conceptual DL model training if any)

ensemble:
  n_folds_for_oof: 5 # As per spec for meta-learner
  train_lgbm: true
  train_teco: true  # Will use dummy predictions
  train_stm_gnn: true # Will use dummy predictions
  train_meta_learner: true
  lgbm_params: # As per spec for AUROC
    num_leaves: 10000
    class_weight: 'balanced'
    num_boost_round: 1000 # Default, can be tuned
    early_stopping_rounds: 50 # Default, can be tuned
    # model_specific_params: {'learning_rate': 0.05} # Example for custom internal lgbm params
    save_path: lgbm_final.joblib # Path to save the final model trained on full data (conceptual)
  teco_params: # As per spec for AUROC
      # Conceptual parameters, assuming TECOTransformerModel takes these
    input_feature_dim: 10 # Placeholder, should match actual data feature dim for TECO (updated by script from dummy_data_features)
    d_model: 512
    num_encoder_layers: 4
    nhead: 8 # Typical for d_model=512
    dim_feedforward: 2048 # Typical for d_model=512
    dropout: 0.1
    num_classes: 2 # Placeholder, should match dummy_data_classes (updated by script)
    max_seq_len: 100 # Example, should match data
    save_path: teco_final.pth # Conceptual save path
  stm_gnn_params: # As per spec for AUROC
    # Conceptual parameters, assuming STMGNN or ModelWithSTMGNNLayer takes these
    num_node_features: 10 # Placeholder, should match actual data (updated by script from dummy_data_features)
    layer_hidden_dim: 256 # For STMGNNLayer consistency
    gnn_output_dim: 256   # For STMGNNLayer consistency
    num_gnn_layers: 5
    global_memory_dim: 128
    num_memory_slots: 10 # Example
    num_heads: 8
    dropout: 0.1
    num_classes: 2 # Placeholder, should match dummy_data_classes (updated by script)
    save_path: stm_gnn_final.pth # Conceptual save path
  meta_learner_xgb_params: # As per spec for AUROC
    depth: 3
    num_boost_round: 200 # Default, can be tuned
    early_stopping_rounds: 20 # Default, can be tuned
    # model_specific_params: {'eta': 0.05} # Example for custom internal xgb params
    save_path: meta_learner_xgb.joblib
  soft_vote_weights: # As per spec for AUROC
    stm_gnn: 0.5
    lgbm: 0.3
    teco: 0.2

# This section would be added/used by evaluate.py
# evaluate_params:
#   model_to_evaluate: 'meta_learner' # 'lgbm', 'stm_gnn', 'teco_transformer'
#   dummy_test_samples: 50
#   apply_calibration: true
#   run_shap: true
#   run_attention_rollout: false