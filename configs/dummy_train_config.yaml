random_seed: 42
use_gpu: true

# --- Data Configuration ---
# Set to true to use dummy data generation specified below
use_dummy_data_for_full_run: false # SET TO FALSE TO USE REAL DATA
data_dir: "data/" # Base directory for trainData.csv, valData.csv
data_paths:
  train: "trainData.csv"
  val: "valData.csv"
  # test: "testData.csv" # Not used in NCV training script directly
  target_column: "outcomeType" # Name of the target variable column in your CSVs

# Dummy data settings (only used if use_dummy_data_for_full_run is true)
dummy_data_total_samples: 1000 # Total samples for the combined (train+val) dummy dataset
dummy_data_features: 50      # Number of features for dummy data
dummy_data_classes: 2        # Number of classes for dummy data
dummy_data_weights: [0.85, 0.15] # Class imbalance for dummy data

# --- Preprocessing Configuration ---
preprocessing:
  numerical_cols: [
    "requestDate",
    "admissionDate",
    "patientAge",
    "glasgowScale",
    "hematocrit",
    "hemoglobin",
    "leucocitos",
    "lymphocytes",
    "urea",
    "creatinine",
    "platelets",
    "diuresis",
    "lengthofStay"
  ] # List of numerical column names. If empty, inferred from dtype.
                     # Example: ['age', 'lab_value_1']
  categorical_cols: [
    "requestType",
    "requestBedType",
    "admissionBedType",
    "admissionHealthUnit",
    "patientGender",
    "patientFfederalUnit",
    "icdCode",
    "blodPressure" # Assuming blodPressure might contain non-numeric strings or is better treated as categorical
  ] # List of categorical column names. If empty, inferred.
                       # Example: ['gender', 'ethnicity']
  imputation_strategy: 'median' # For numerical: 'mean', 'median', 'most_frequent'
                                # For categorical: 'most_frequent' or a constant like 'missing'
  scale_numerics: true
  onehot_handle_unknown: 'ignore' # For OneHotEncoder: 'error', 'ignore', 'infrequent_if_exist'

# --- Balancing Configuration ---
balancing:
  use_rsmote_gan_in_cv: true # Whether to apply RSMOTE-GAN in inner CV folds
  rsmote_gan_params:
    k: 5
    minority_upsample_factor: 3.0 # Target factor for minority class size relative to original

# --- Ensemble Configuration ---
ensemble:
  n_outer_folds: 5 # Number of outer folds for NCV
  n_inner_folds_for_oof: 5 # Number of inner folds for OOF generation

  train_lgbm: true
  train_teco: true
  train_stm_gnn: false # STM-GNN is conceptual and has been removed from training script
  train_meta_learner: true

  lgbm_params:
    num_leaves: 10000 # As per AUROC spec; consider tuning
    class_weight: 'balanced'
    num_boost_round: 1500 # Increased for full run
    early_stopping_rounds: 100 # Increased for full run
    # model_specific_params: { 'learning_rate': 0.01, 'reg_alpha': 0.1, 'reg_lambda': 0.1 } # Example further tuning
    save_path: 'models_output/lgbm_final_ncv_trained.joblib' # Path for potential final model save (not directly used by NCV script)

  teco_params:
    # feature_columns_teco: # Not needed here, derived from preprocessor output in train script
    # target_column_teco: # Not needed here, derived from preprocessor output in train script

    # Model hyperparameters (as per AUROC spec, but consider tuning)
    input_feature_dim: null # Will be determined by preprocessed data in train script
    d_model: 512
    num_encoder_layers: 4
    nhead: 8
    dim_feedforward: 2048
    dropout: 0.1
    num_classes: null # Will be determined by data in train script
    max_seq_len: 150 # Adjust based on typical sequence length in your data (if applicable, current is 1 for tabular)

    # Training hyperparameters for TECO (per inner fold)
    epochs_teco_inner: 20 # Increased epochs for TECO training in inner folds
    batch_size_teco: 32
    lr_teco: 0.0001
    # class_weights_teco: [1.0, 3.0] # Example if using weighted loss for TECO, must match num_classes
    save_path: 'models_output/teco_final_ncv_trained.pth'

  stm_gnn_params: # These are highly conceptual as STM-GNN requires graph data
    num_node_features: null # Placeholder, will depend on graph feature engineering
    layer_hidden_dim: 256
    gnn_output_dim: 256   # Not directly used by classifier if pooling from layer_hidden_dim
    num_gnn_layers: 5
    global_memory_dim: 128
    num_memory_slots: 10
    num_heads: 8
    dropout: 0.1
    num_classes: null # Placeholder, will be set from data
    # Graph-specific params:
    # num_snapshots_stm: 1 # If processing single graph snapshot from tabular row
    # epochs_stm_gnn_inner: 15
    # lr_stm_gnn: 0.0005
    save_path: 'models_output/stm_gnn_final_ncv_trained.pth'

  meta_learner_xgb_params:
    depth: 3 # As per AUROC spec
    num_boost_round: 250 # Increased for full run
    early_stopping_rounds: 30 # Increased for full run
    # model_specific_params: { 'eta': 0.01, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8 } # Example
    save_path: 'models_output/meta_learner_xgb_ncv_trained.joblib'

  soft_vote_weights: # As per AUROC spec (STM-GNN removed)
    lgbm: 0.6 # Adjusted after removing STM-GNN, ensure weights make sense
    teco: 0.4 # Adjusted after removing STM-GNN

# --- W&B Logging ---
wandb:
  project: 'ifbme-project-full-run' # Update project name for full runs
  entity: null # Replace with your W&B username or team name
  run_name: 'full_ncv_ensemble_run' # Descriptive run name
  run_notes: 'Full Nested CV training run with real data (if configured) and updated parameters.'
  tags: ['full_run', 'ncv', 'ensemble']